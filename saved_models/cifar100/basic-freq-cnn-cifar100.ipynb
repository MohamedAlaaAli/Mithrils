{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e301c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T06:59:42.580437Z",
     "iopub.status.busy": "2024-09-04T06:59:42.580058Z",
     "iopub.status.idle": "2024-09-04T07:00:03.235193Z",
     "shell.execute_reply": "2024-09-04T07:00:03.234281Z"
    },
    "papermill": {
     "duration": 20.661982,
     "end_time": "2024-09-04T07:00:03.237682",
     "exception": false,
     "start_time": "2024-09-04T06:59:42.575700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary \n",
    "from torchmetrics import Accuracy, F1Score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4781db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T07:00:03.246148Z",
     "iopub.status.busy": "2024-09-04T07:00:03.245279Z",
     "iopub.status.idle": "2024-09-04T07:00:11.751114Z",
     "shell.execute_reply": "2024-09-04T07:00:11.750061Z"
    },
    "papermill": {
     "duration": 8.512446,
     "end_time": "2024-09-04T07:00:11.753780",
     "exception": false,
     "start_time": "2024-09-04T07:00:03.241334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Image size: torch.Size([12, 112, 112])\n",
      "Train size: 40000\n",
      "Validation size: 10000\n",
      "Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),          \n",
    "    transforms.RandomHorizontalFlip(),      \n",
    "    transforms.RandomRotation(15),          \n",
    "    transforms.ToTensor()                   \n",
    "])\n",
    "\n",
    "test_val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),          \n",
    "    transforms.ToTensor()                  \n",
    "])\n",
    "\n",
    "\n",
    "class FFTShiftedMNIST(Dataset):\n",
    "    def __init__(self,  base_transform, train_flag, fft_crop_size =None):\n",
    "        self.dataset = datasets.CIFAR100(root='./data', train=train_flag, download=True, transform=base_transform)\n",
    "        self.fft_crop_size = fft_crop_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        image = image.to(\"cuda\")\n",
    "        \n",
    "        # Apply FFT, shift, and get magnitude and phase\n",
    "        fft = torch.fft.fft2(image)  # Remove single channel dimension\n",
    "        fft_shifted = torch.fft.fftshift(fft)\n",
    "        real = torch.real(fft_shifted)\n",
    "        imag = torch.imag(fft_shifted)     \n",
    "        magnitude = torch.abs(fft_shifted)\n",
    "        phase = torch.angle(fft_shifted)\n",
    "        \n",
    "        if self.fft_crop_size is not None:\n",
    "            # Calculate the center crop region\n",
    "            center_x, center_y = magnitude.shape[1] // 2, magnitude.shape[2] // 2\n",
    "            crop_size = self.fft_crop_size // 2\n",
    "            \n",
    "            # Crop the magnitude and phase around the center\n",
    "            imag = imag[:, center_x - crop_size:center_x + crop_size, center_y - crop_size:center_y + crop_size]\n",
    "            real = real[:, center_x - crop_size:center_x + crop_size, center_y - crop_size:center_y + crop_size]\n",
    "            magnitude = magnitude[:, center_x - crop_size:center_x + crop_size, center_y - crop_size:center_y + crop_size]\n",
    "            phase = phase[:, center_x - crop_size:center_x + crop_size, center_y - crop_size:center_y + crop_size]\n",
    "\n",
    "        \n",
    "        # Stack magnitude and phase along the channel dimension\n",
    "        transformed_image = torch.cat((magnitude, phase, real, imag), dim=0) \n",
    "        \n",
    "        return transformed_image, label\n",
    "\n",
    "# Create dataset instances for train, validation, and test sets\n",
    "train_dataset = FFTShiftedMNIST(train_transform,train_flag=True, fft_crop_size = 112)\n",
    "test_dataset = FFTShiftedMNIST(test_val_transform,train_flag=False, fft_crop_size = 112)\n",
    "\n",
    "# Split the training dataset into train and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Image size: {next(iter(train_dataset.dataset))[0].shape}\")\n",
    "print(f\"Train size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation size: {len(val_loader.dataset)}\")\n",
    "print(f\"Test size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abd4dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T07:00:11.773086Z",
     "iopub.status.busy": "2024-09-04T07:00:11.772334Z",
     "iopub.status.idle": "2024-09-04T07:00:11.783479Z",
     "shell.execute_reply": "2024-09-04T07:00:11.782530Z"
    },
    "papermill": {
     "duration": 0.023262,
     "end_time": "2024-09-04T07:00:11.785987",
     "exception": false,
     "start_time": "2024-09-04T07:00:11.762725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(12, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), \n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv2d(64,128 , kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Dropout(0.2)\n",
    "        ) \n",
    "        self.classifier = nn.Linear(256, num_classes) \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to shape (batch_size, 256)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfa4d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T07:00:11.804673Z",
     "iopub.status.busy": "2024-09-04T07:00:11.804341Z",
     "iopub.status.idle": "2024-09-04T07:00:12.403992Z",
     "shell.execute_reply": "2024-09-04T07:00:12.403022Z"
    },
    "papermill": {
     "duration": 0.611668,
     "end_time": "2024-09-04T07:00:12.406540",
     "exception": false,
     "start_time": "2024-09-04T07:00:11.794872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]           3,488\n",
      "               ELU-2         [-1, 32, 112, 112]               0\n",
      "       BatchNorm2d-3         [-1, 32, 112, 112]              64\n",
      "         MaxPool2d-4           [-1, 32, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          18,496\n",
      "               ELU-6           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "         MaxPool2d-8           [-1, 64, 28, 28]               0\n",
      "            Conv2d-9          [-1, 128, 28, 28]          73,856\n",
      "              ELU-10          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-11          [-1, 128, 28, 28]             256\n",
      "        MaxPool2d-12          [-1, 128, 14, 14]               0\n",
      "           Conv2d-13          [-1, 256, 14, 14]         295,168\n",
      "              ELU-14          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-15          [-1, 256, 14, 14]             512\n",
      "AdaptiveAvgPool2d-16            [-1, 256, 1, 1]               0\n",
      "          Dropout-17            [-1, 256, 1, 1]               0\n",
      "           Linear-18                  [-1, 100]          25,700\n",
      "================================================================\n",
      "Total params: 417,668\n",
      "Trainable params: 417,668\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 18.57\n",
      "Params size (MB): 1.59\n",
      "Estimated Total Size (MB): 20.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "freq_model = BasicNet(100)\n",
    "device = torch.device(\"cuda\")\n",
    "freq_model.to(device)\n",
    "summary(freq_model, (12, 112, 112))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f327709",
   "metadata": {},
   "source": [
    "## Training & Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd73d6a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T07:00:12.420065Z",
     "iopub.status.busy": "2024-09-04T07:00:12.419705Z",
     "iopub.status.idle": "2024-09-04T07:00:12.424961Z",
     "shell.execute_reply": "2024-09-04T07:00:12.424122Z"
    },
    "papermill": {
     "duration": 0.014416,
     "end_time": "2024-09-04T07:00:12.427031",
     "exception": false,
     "start_time": "2024-09-04T07:00:12.412615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(freq_model.parameters(), lr = 0.005) \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b825a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T07:00:12.440804Z",
     "iopub.status.busy": "2024-09-04T07:00:12.440476Z",
     "iopub.status.idle": "2024-09-04T09:06:01.989250Z",
     "shell.execute_reply": "2024-09-04T09:06:01.988204Z"
    },
    "papermill": {
     "duration": 7549.569669,
     "end_time": "2024-09-04T09:06:02.002838",
     "exception": false,
     "start_time": "2024-09-04T07:00:12.433169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1\\200], train-loss: 3.8319502937316896, train-acc: 0.10300000011920929, val-loss: 5.315420794638858, val-acc: 0.05420000106096268\n",
      "[2\\200], train-loss: 3.2116633590698243, train-acc: 0.20607499778270721, val-loss: 5.142668377821613, val-acc: 0.07649999856948853\n",
      "[3\\200], train-loss: 2.9757780509948732, train-acc: 0.2522749900817871, val-loss: 6.153409222888339, val-acc: 0.061799999326467514\n",
      "[4\\200], train-loss: 2.8316635913848875, train-acc: 0.27992498874664307, val-loss: 3.8336095081013477, val-acc: 0.1671999990940094\n",
      "[5\\200], train-loss: 2.7222075000762938, train-acc: 0.30172500014305115, val-loss: 3.8707129256740496, val-acc: 0.19589999318122864\n",
      "[6\\200], train-loss: 2.629307083129883, train-acc: 0.32374998927116394, val-loss: 4.7485187387770145, val-acc: 0.13580000400543213\n",
      "[7\\200], train-loss: 2.5593336771011352, train-acc: 0.3386000096797943, val-loss: 2.970239580057229, val-acc: 0.27880001068115234\n",
      "[8\\200], train-loss: 2.4978007675170897, train-acc: 0.34942498803138733, val-loss: 3.4447419278940576, val-acc: 0.22709999978542328\n",
      "[9\\200], train-loss: 2.4410215644836426, train-acc: 0.3621000051498413, val-loss: 2.5468795086927476, val-acc: 0.34200000762939453\n",
      "[10\\200], train-loss: 2.394134477043152, train-acc: 0.37252500653266907, val-loss: 3.6580763713569397, val-acc: 0.249099999666214\n",
      "[11\\200], train-loss: 2.357389785194397, train-acc: 0.37940001487731934, val-loss: 2.812165477473265, val-acc: 0.3100999891757965\n",
      "[12\\200], train-loss: 2.3188487880706785, train-acc: 0.3870750069618225, val-loss: 2.78078588862328, val-acc: 0.30820000171661377\n",
      "[13\\200], train-loss: 2.2791145875930785, train-acc: 0.39800000190734863, val-loss: 2.4828081510628865, val-acc: 0.36739999055862427\n",
      "[14\\200], train-loss: 2.244733791732788, train-acc: 0.4078249931335449, val-loss: 3.0093688144805326, val-acc: 0.3073999881744385\n",
      "[15\\200], train-loss: 2.202971369171143, train-acc: 0.4152500033378601, val-loss: 3.1550297904166444, val-acc: 0.289000004529953\n",
      "[16\\200], train-loss: 2.176641332054138, train-acc: 0.42089998722076416, val-loss: 2.71277398820136, val-acc: 0.3296000063419342\n",
      "[17\\200], train-loss: 2.1490592321395874, train-acc: 0.4281249940395355, val-loss: 3.1685605398408927, val-acc: 0.2840999960899353\n",
      "[18\\200], train-loss: 2.1242379302978516, train-acc: 0.4313249886035919, val-loss: 2.624567850380187, val-acc: 0.3407000005245209\n",
      "[19\\200], train-loss: 2.09284714012146, train-acc: 0.43860000371932983, val-loss: 2.7949335924379386, val-acc: 0.33399999141693115\n",
      "[20\\200], train-loss: 1.9188870946884156, train-acc: 0.4801749885082245, val-loss: 2.1059546022658138, val-acc: 0.4426000118255615\n",
      "[21\\200], train-loss: 1.8591187425613402, train-acc: 0.4936999976634979, val-loss: 2.0960582692152374, val-acc: 0.4499000012874603\n",
      "[22\\200], train-loss: 1.8440703325271606, train-acc: 0.5002250075340271, val-loss: 2.09026563319431, val-acc: 0.44699999690055847\n",
      "[23\\200], train-loss: 1.8301378923416138, train-acc: 0.5034999847412109, val-loss: 2.0975259375420348, val-acc: 0.45210000872612\n",
      "[24\\200], train-loss: 1.818909794998169, train-acc: 0.5043500065803528, val-loss: 2.0728596889289324, val-acc: 0.45669999718666077\n",
      "[25\\200], train-loss: 1.8103647150039672, train-acc: 0.5087000131607056, val-loss: 2.073976005718207, val-acc: 0.45590001344680786\n",
      "[26\\200], train-loss: 1.8025882572174072, train-acc: 0.5075250267982483, val-loss: 2.083559386289803, val-acc: 0.45509999990463257\n",
      "[27\\200], train-loss: 1.7902693908691407, train-acc: 0.5120499730110168, val-loss: 2.084077352171491, val-acc: 0.45210000872612\n",
      "[28\\200], train-loss: 1.7898472787857056, train-acc: 0.5113250017166138, val-loss: 2.0759514760059914, val-acc: 0.45570001006126404\n",
      "[29\\200], train-loss: 1.7845710891723632, train-acc: 0.511900007724762, val-loss: 2.0651388289822137, val-acc: 0.45879998803138733\n",
      "[30\\200], train-loss: 1.7724508317947387, train-acc: 0.5140249729156494, val-loss: 2.0707989483122615, val-acc: 0.45809999108314514\n",
      "[31\\200], train-loss: 1.7677457305908204, train-acc: 0.5163999795913696, val-loss: 2.084215150517263, val-acc: 0.45590001344680786\n",
      "[32\\200], train-loss: 1.7638950651168823, train-acc: 0.5164750218391418, val-loss: 2.0734752378645975, val-acc: 0.46149998903274536\n",
      "[33\\200], train-loss: 1.7515272096633911, train-acc: 0.5197499990463257, val-loss: 2.067212239951844, val-acc: 0.46480000019073486\n",
      "[34\\200], train-loss: 1.7504335544586183, train-acc: 0.5216249823570251, val-loss: 2.061753894872726, val-acc: 0.460099995136261\n",
      "[35\\200], train-loss: 1.7398325414657594, train-acc: 0.5237749814987183, val-loss: 2.063714596116619, val-acc: 0.4593000113964081\n",
      "[36\\200], train-loss: 1.7337781576156617, train-acc: 0.524150013923645, val-loss: 2.0623962742507835, val-acc: 0.45590001344680786\n",
      "[37\\200], train-loss: 1.7199906393051148, train-acc: 0.5284749865531921, val-loss: 2.066683938548823, val-acc: 0.4636000096797943\n",
      "[38\\200], train-loss: 1.7246053537368775, train-acc: 0.5271250009536743, val-loss: 2.058941847199847, val-acc: 0.4596000015735626\n",
      "[39\\200], train-loss: 1.7134167623519898, train-acc: 0.529449999332428, val-loss: 2.0523889732968277, val-acc: 0.46549999713897705\n",
      "[40\\200], train-loss: 1.7070075588226319, train-acc: 0.5279499888420105, val-loss: 2.0532073860715148, val-acc: 0.4627000093460083\n",
      "[41\\200], train-loss: 1.704923454284668, train-acc: 0.5319499969482422, val-loss: 2.0557417998647995, val-acc: 0.4611999988555908\n",
      "[42\\200], train-loss: 1.6920114931106567, train-acc: 0.5367249846458435, val-loss: 2.0654509667378322, val-acc: 0.4648999869823456\n",
      "[43\\200], train-loss: 1.6905292530059814, train-acc: 0.5364000201225281, val-loss: 2.071705469659939, val-acc: 0.4609000086784363\n",
      "[44\\200], train-loss: 1.684036817741394, train-acc: 0.5352749824523926, val-loss: 2.0603906994412657, val-acc: 0.4650999903678894\n",
      "[45\\200], train-loss: 1.6728235914230347, train-acc: 0.5369250178337097, val-loss: 2.058826208114624, val-acc: 0.46369999647140503\n",
      "[46\\200], train-loss: 1.6532194536209106, train-acc: 0.5447750091552734, val-loss: 2.0519673209281484, val-acc: 0.46939998865127563\n",
      "[47\\200], train-loss: 1.6503476985931396, train-acc: 0.5447999835014343, val-loss: 2.055229161954989, val-acc: 0.4659999907016754\n",
      "[48\\200], train-loss: 1.6471224979400634, train-acc: 0.5453000068664551, val-loss: 2.037587774027685, val-acc: 0.47040000557899475\n",
      "[49\\200], train-loss: 1.648433821105957, train-acc: 0.5449749827384949, val-loss: 2.049559713928563, val-acc: 0.46299999952316284\n",
      "[50\\200], train-loss: 1.6424532203674316, train-acc: 0.5451750159263611, val-loss: 2.052916125886759, val-acc: 0.4648999869823456\n",
      "[51\\200], train-loss: 1.644430976676941, train-acc: 0.5456749796867371, val-loss: 2.0540402520234418, val-acc: 0.4666999876499176\n",
      "[52\\200], train-loss: 1.643360242652893, train-acc: 0.5473999977111816, val-loss: 2.037977706095216, val-acc: 0.47200000286102295\n",
      "[53\\200], train-loss: 1.640903125190735, train-acc: 0.5471000075340271, val-loss: 2.0507116970742585, val-acc: 0.4661000072956085\n",
      "[54\\200], train-loss: 1.6393988040924072, train-acc: 0.5446249842643738, val-loss: 2.047542634283661, val-acc: 0.4717999994754791\n",
      "[55\\200], train-loss: 1.6386487895965576, train-acc: 0.5468249917030334, val-loss: 2.045773360379942, val-acc: 0.4699999988079071\n",
      "[56\\200], train-loss: 1.6357925333023071, train-acc: 0.5489000082015991, val-loss: 2.0348071323078907, val-acc: 0.46939998865127563\n",
      "[57\\200], train-loss: 1.638040135192871, train-acc: 0.5475999712944031, val-loss: 2.0513815712776915, val-acc: 0.47029998898506165\n",
      "[58\\200], train-loss: 1.6372540395736694, train-acc: 0.5462250113487244, val-loss: 2.046782548260537, val-acc: 0.47110000252723694\n",
      "[59\\200], train-loss: 1.638668667602539, train-acc: 0.5479999780654907, val-loss: 2.0430293364130008, val-acc: 0.4715999960899353\n",
      "[60\\200], train-loss: 1.6399672136306762, train-acc: 0.5463749766349792, val-loss: 2.0456246540045284, val-acc: 0.4706000089645386\n",
      "[61\\200], train-loss: 1.6440132698059082, train-acc: 0.5472249984741211, val-loss: 2.054395403072333, val-acc: 0.46389999985694885\n",
      "[62\\200], train-loss: 1.6360464710235596, train-acc: 0.5470499992370605, val-loss: 2.041796776139812, val-acc: 0.4706000089645386\n",
      "[63\\200], train-loss: 1.638611248588562, train-acc: 0.5493500232696533, val-loss: 2.0438941914564484, val-acc: 0.4668999910354614\n",
      "[64\\200], train-loss: 1.643443353652954, train-acc: 0.5442500114440918, val-loss: 2.034593157707506, val-acc: 0.46880000829696655\n",
      "[65\\200], train-loss: 1.6305530597686768, train-acc: 0.5505499839782715, val-loss: 2.0284473410077917, val-acc: 0.4690000116825104\n",
      "[66\\200], train-loss: 1.6334961437225342, train-acc: 0.5477749705314636, val-loss: 2.0429382688680273, val-acc: 0.47189998626708984\n",
      "[67\\200], train-loss: 1.6319275762557983, train-acc: 0.5507500171661377, val-loss: 2.0402909289499758, val-acc: 0.4699000120162964\n",
      "[68\\200], train-loss: 1.6339485921859742, train-acc: 0.5464249849319458, val-loss: 2.047445872786698, val-acc: 0.46970000863075256\n",
      "[69\\200], train-loss: 1.6312611671447754, train-acc: 0.5491499900817871, val-loss: 2.045843828255963, val-acc: 0.4650999903678894\n",
      "[70\\200], train-loss: 1.634439529800415, train-acc: 0.550000011920929, val-loss: 2.0354136911926757, val-acc: 0.4706999957561493\n",
      "[71\\200], train-loss: 1.6345929096221923, train-acc: 0.546999990940094, val-loss: 2.0411624680658815, val-acc: 0.47029998898506165\n",
      "[72\\200], train-loss: 1.635969973373413, train-acc: 0.5457500219345093, val-loss: 2.0473349914429293, val-acc: 0.46639999747276306\n",
      "[73\\200], train-loss: 1.6437476360321044, train-acc: 0.5465750098228455, val-loss: 2.039378525345189, val-acc: 0.4641999900341034\n",
      "[74\\200], train-loss: 1.6399447513580323, train-acc: 0.5456249713897705, val-loss: 2.033759417047926, val-acc: 0.47200000286102295\n",
      "[75\\200], train-loss: 1.6366137105941772, train-acc: 0.5491999983787537, val-loss: 2.042994274455271, val-acc: 0.47360000014305115\n",
      "Early stopping triggered after 75 epochs\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'baseline_cnn_freq'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "train_cost, val_cost = [],[]\n",
    "train_acc, val_acc = [],[]\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0 \n",
    "    acc_train = Accuracy('multiclass', num_classes=100).to(device)\n",
    "    freq_model.train().cuda()  # set the model for training \n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = freq_model(images)\n",
    "        curr_loss = criterion(pred, labels)\n",
    "        train_loss += curr_loss.item()\n",
    "\n",
    "        curr_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_train(pred, labels)\n",
    "        \n",
    "    train_cost.append(train_loss / len(train_loader))\n",
    "    train_acc.append(acc_train.compute()) \n",
    "\n",
    "\n",
    "    val_loss = 0 \n",
    "    acc_val = Accuracy(task=\"multiclass\", num_classes=100).to(device)\n",
    "    freq_model.eval().cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            pred = freq_model(images)\n",
    "\n",
    "            curr_loss = criterion(pred, labels)\n",
    "            val_loss += curr_loss.item()\n",
    "\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            acc_val(predicted, labels)\n",
    "\n",
    "    val_cost.append(val_loss / len(val_loader))\n",
    "    val_acc.append(acc_val.compute())\n",
    "\n",
    "    print(f\"[{epoch+1}\\{EPOCHS}], train-loss: {train_cost[-1]}, train-acc: {acc_train.compute()}, val-loss: {val_cost[-1]}, val-acc: {acc_val.compute()}\")\n",
    "#     torch.save(freq_model.state_dict(), f'baseline_cnn_freq/checkpoint_{epoch + 1}')\n",
    "\n",
    "    scheduler.step(val_cost[-1])\n",
    "\n",
    "    if val_cost[-1] < best_val_loss:\n",
    "        best_val_loss = val_cost[-1]\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "torch.save(freq_model.state_dict(), 'baseline_cnn_freq/freq_model_200_EPOCHS.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19f92a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bfa3016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_freq_model = BasicNet(100)\n",
    "final_freq_model.load_state_dict(torch.load(\"freq_model_200_EPOCHS.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db71e5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.458\n",
      "F1 : 0.458\n"
     ]
    }
   ],
   "source": [
    "acc = Accuracy(task=\"multiclass\", num_classes=100).to(device)\n",
    "f1 = F1Score('multiclass', num_classes=100).to(device)\n",
    "\n",
    "final_freq_model.eval().cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        predictions = final_freq_model(images) \n",
    "\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "\n",
    "        acc(predicted, labels)\n",
    "        f1(predicted, labels)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {acc.compute().data:.3f}\")\n",
    "print(f\"F1 : {f1.compute().data:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7583.550922,
   "end_time": "2024-09-04T09:06:03.335479",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-04T06:59:39.784557",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

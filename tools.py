# -*- coding: utf-8 -*-
"""tools

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/tools-51cae0b3-7987-4702-b07e-7cc5f94ce801.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240905/auto/storage/goog4_request%26X-Goog-Date%3D20240905T011825Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da96f0891cf7b28febf1f4156ba095281c496f86c7ddd19df66a3be32cdd5a602257d5f2f0f5620e5c0251499d4dfea0103c89e7ca30eeb4f228c5a72637325f293349108afac630a1d4c30f6bf6759613ae7b3880a890abf514c03e5e17626d0811fc1d50c1b3e29a4c6d99bc62e0a260c8be5d8295804b8df628c16510150633b612d27f8c4e44178b41caa54742a47a06241fd6aec49d10f9e5b75084eeb1bf0c206e7860e24f54770ed34e1461b2c69da88814b8ad2907cd21e4f312640803185bb1af2eaabb6fbe5d3819a410a64144a2b02efbcdb7756387c9db1756c51aad966ebc3d8c1b94791f74da570d971200c2da5f40c311564e8266caaa55732
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score
import torch
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import datasets, transforms
import torch.fft
import random
from tqdm import tqdm
import torchvision.models as models

def set_seed(seed):
    """
        Set the seed for random number generation to ensure reproducibility.

        This function sets the seed for various libraries used in machine learning and
        scientific computing to ensure that results are consistent across runs. It affects
        the following libraries and components:

            - `random`: Python's built-in random number generator
            - `numpy`: NumPy's random number generator
            - `torch`: PyTorch's random number generator for CPU
            - `torch.cuda`: PyTorch's random number generator for GPU
            - `torch.backends.cudnn`: Controls the behavior of cuDNN, a GPU-accelerated library for deep neural networks

        Parameters:
            seed (int): The seed value to be used for random number generation.

        Returns:
            None
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


set_seed(seed=42)

def prepare_transforms():
    """
        Prepare and return the data transformation pipelines for training and base (validation/testing).

        This function defines and returns two sets of image transformations:

            - `train_transforms`: Used for training data, including resizing, random horizontal flipping,
              random rotation, and conversion to tensor.
            - `base_transforms`: Used for base transformations applied to validation/testing data,
              including resizing and conversion to tensor.

        Returns:
            tuple: A tuple containing two transformation pipelines:
                - train_transforms (transforms.Compose): The transformations applied to training images.
                - base_transforms (transforms.Compose): The transformations applied to validation/testing images.
    """

    base_transforms = transforms.Compose([
            transforms.Resize((224, 224)),  # Resize to 224x224
            transforms.ToTensor(),
            ])

    train_transforms = transforms.Compose([
            transforms.Resize((224,224)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(25),
            transforms.ToTensor()
        ])

    return train_transforms, base_transforms

class CustomDataset(Dataset):
    def __init__(self, dataset_name:str, domain:str, train:bool, transform=None, device="cuda"):
        """
            Initialize a dataset object with specified configurations.

            This constructor method initializes the dataset based on the provided `dataset_name` and other parameters.
            It supports MNIST, CIFAR-10, and CIFAR-100 datasets. The dataset is downloaded and transformed according
            to the specified parameters.

            Parameters:
                dataset_name (str): The name of the dataset to load. Supported values are 'mnist', 'cifar10', and 'cifar100'.
                domain (str): The domain of the dataset. This parameter is currently unused but included for potential future use.
                train (bool): If True, load the training set; if False, load the test set.
                transform (callable, optional): A function/transform to apply to the dataset. Default is None.
                device (str, optional): The device to use for computations. Default is 'cuda'.

            Raises:
                ValueError: If `dataset_name` is not one of the supported values ('mnist', 'cifar10', 'cifar100').

            Attributes:
                device (str): The device used for computations.
                domain (str): The domain of the dataset.
                transform (callable, optional): The function/transform applied to the dataset.
                dataset (torchvision.datasets): The loaded dataset.
        """
        self.device = device
        self.domain = domain
        self.transform = transform

        if dataset_name == "mnist":
            self.dataset = datasets.MNIST(root='./data', train=train, download=True, transform=self.transform)
        elif dataset_name == "cifar10":
            self.dataset = datasets.CIFAR10(root='./data', train=train, download=True, transform=self.transform)
        elif dataset_name == "cifar100":
            self.dataset = datasets.CIFAR100(root='./data', train=train, download=True, transform=self.transform)
        else:
            raise ValueError("Dataset not supported. Choose from 'mnist', 'cifar10', or 'cifar100'.")

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        """
            Retrieve an item from the dataset at the specified index.

            This method fetches an image and its corresponding label from the dataset. If the `domain` is set to
            "freq", the image is transformed to the frequency domain using a Fourier transform.

            Parameters:
                idx (int): The index of the item to retrieve from the dataset.

            Returns:
                tuple: A tuple containing:
                    - image (torch.Tensor): The image at the specified index, potentially transformed to the frequency domain.
                    - label (int): The label corresponding to the image at the specified index.

            Notes:
                - The image is moved to the device specified during initialization.
                - If `self.domain` is set to "freq", the image is transformed to the frequency domain using the `self.fourier` function.
        """
        image, label = self.dataset[idx]
        image = image.to(self.device)

        if self.domain == "freq":
            image = self.fourier(image).squeeze(0)

        return image, label

    def fourier(self, image):
        """
            Transform the input image to grayscale if it is in RGB format.

            This method converts RGB images to grayscale using predefined weights and removes the channel dimension.
            If the image is already in grayscale, it is returned unchanged.

            Parameters:
                image (torch.Tensor): The input image tensor with shape [C, H, W] where C is the number of channels.

            Returns:
                torch.Tensor: The grayscale image tensor with shape [H, W] if the input was RGB, or the original tensor if already grayscale.

            Notes:
                - The method uses the following weights for RGB to grayscale conversion: [0.2989, 0.5870, 0.1140].
                - The conversion is performed by taking a weighted sum of the RGB channels.
        """

        if image.shape[0]==3:
            weights = torch.tensor([0.2989, 0.5870, 0.1140], device=self.device).view(1, 3, 1, 1)
            grayscale_tensor = (image * weights).sum(dim=1, keepdim=False)

        else:
            grayscale_tensor = image

        def process_patches(tensor, patch_size):
            """
                Divide an image tensor into patches and apply a 2D Fourier Transform to each patch.

                This function takes an image tensor, divides it into smaller patches of size `patch_size x patch_size`,
                applies a 2D Fourier Transform to each patch, and returns the magnitude of the transformed patches.

                Parameters:
                    tensor (torch.Tensor): The input image tensor with shape [C, H, W]. Assumes grayscale images with C=1.
                    patch_size (int): The size of the patches to extract from the image.

                Returns:
                    torch.Tensor: The magnitude of the Fourier Transform of the patches, with the same height and width as the input tensor.

                Notes:
                    - The function uses 2D FFT and shifts the zero-frequency component to the center of the spectrum.
                    - The resulting tensor has the same dimensions as the input tensor but with the frequency information represented.
            """

            # Divide into patches of size patch_size x patch_size
            patches = tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)
            # patches shape: [64, 224//patch_size, 224//patch_size, patch_size, patch_size]

            # Apply 2D Fourier Transform to each patch
            fft = torch.fft.fft2(patches)
            fft_shifted = torch.fft.fftshift(fft)
            magnitude = torch.abs(fft_shifted)

            # Flatten the patch dimensions back to image dimensions
            magnitude = magnitude.view(tensor.size(0), tensor.size(1) // patch_size, tensor.size(2) // patch_size, patch_size, patch_size)
            magnitude = magnitude.permute(0, 1, 3, 2, 4).contiguous()
            magnitude = magnitude.view(tensor.size(0), tensor.size(1), tensor.size(2))

            return magnitude

        # Process patches of different sizes
        magnitude_2x2 = process_patches(grayscale_tensor, 2)  # [64, 224, 224]
        magnitude_4x4 = process_patches(grayscale_tensor, 4)  # [64, 224, 224]
        magnitude_8x8 = process_patches(grayscale_tensor, 8)  # [64, 224, 224]

        # Stack the magnitudes along a new channel dimension
        pyramid_magnitude = torch.stack([magnitude_2x2, magnitude_4x4, magnitude_8x8], dim=1)  # [64, 3, 224, 224]

        return pyramid_magnitude

def create_dataloaders(dataset_name: str, domain: str, batch_size: int = 16, transform_train=None, transform_test=None, device: str = "cuda") -> tuple:
    """
    Create DataLoaders for training, validation, and testing datasets.

    This function sets up DataLoaders for the specified dataset, domain, and data splits. It includes options for applying
    different transformations to training and testing datasets. Additionally, it splits the training dataset into training
    and validation sets.

    Parameters:
        dataset_name (str): The name of the dataset to load. Supported values are 'mnist', 'cifar10', and 'cifar100'.
        domain (str): The domain of the dataset (e.g., "freq"). This parameter is used by the CustomDataset class.
        batch_size (int, optional): The batch size for the DataLoaders. Default is 16.
        transform_train (callable, optional): A function/transform to apply to the training dataset. Must be provided.
        transform_test (callable, optional): A function/transform to apply to the test dataset. If None, uses `transform_train`.
        device (str, optional): The device to use for computations. Default is 'cuda'.

    Returns:
        tuple: A tuple containing:
            - train_loader (DataLoader): DataLoader for the training dataset.
            - val_loader (DataLoader): DataLoader for the validation dataset.
            - test_loader (DataLoader): DataLoader for the test dataset.

    Raises:
        ValueError: If `transform_train` is not provided.

    Notes:
        - The training dataset is split into training and validation sets with an 80-20 split.
        - `transform_test` defaults to `transform_train` if not provided.
    """
    if transform_train is None:
        raise ValueError("You must pass a transform for the training dataset")

    transform_test = transform_test if transform_test else transform_train

    # Create train dataset
    train_dataset = CustomDataset(dataset_name=dataset_name, domain=domain, train=True, transform=transform_train, device=device)

    # Create test dataset
    test_dataset = CustomDataset(dataset_name=dataset_name, domain=domain, train=False, transform=transform_test, device=device)

    # Split train dataset into training and validation sets
    val_size = int(0.2 * len(train_dataset))
    train_size = len(train_dataset) - val_size
    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader

def prep(dataset_name, domain, batch_size, device):
    """
        Prepare DataLoaders for training, validation, and testing datasets with specified configurations.

        This function sets up and returns DataLoaders for the given dataset, domain, and batch size. It first applies the
        transformation functions to the datasets and then creates DataLoaders for the training, validation, and testing splits.

        Parameters:
            dataset_name (str): The name of the dataset to load. Supported values are 'mnist', 'cifar10', and 'cifar100'.
            domain (str): The domain of the dataset (e.g., "freq"). This parameter is used by the CustomDataset class.
            batch_size (int): The batch size for the DataLoaders.
            device (str): The device to use for computations. Default is 'cuda'.

        Returns:
            tuple: A tuple containing:
                - train_loader (DataLoader): DataLoader for the training dataset.
                - val_loader (DataLoader): DataLoader for the validation dataset.
                - test_loader (DataLoader): DataLoader for the test dataset.

        Notes:
            - The function utilizes `prepare_transforms` to get the transformations for training and testing datasets.
            - The `create_dataloaders` function is used to generate the DataLoaders based on the specified dataset and domain.
    """

    train_transforms, test_transforms = prepare_transforms()
    train_loader, val_loader, test_loader = create_dataloaders(
                                                                    dataset_name=dataset_name,
                                                                    domain=domain,
                                                                    batch_size=batch_size,
                                                                    transform_train=train_transforms,
                                                                    transform_test=test_transforms,
                                                                    device=device
                                                                )
    return train_loader, val_loader, test_loader

class ModelLoader:

    def __init__(self, from_hub=False, from_path: str = None, model_instance: torch.nn.Module = None, device: str = "cpu"):
        """
        Initialize the ModelLoader with specified configurations.

        This constructor initializes the model based on the provided parameters, either by loading from a model hub or
        from a local path.

        Parameters:
            from_hub (bool): If True, load the model from a hub using the `model_instance` provided.
            from_path (str, optional): Path to the local model file. This is required if `from_hub` is False.
            model_instance (torch.nn.Module, optional): The model class to instantiate if `from_hub` is True. Must be provided.
            device (str, optional): The device to load the model on. Default is 'cpu'.

        Raises:
            ValueError: If `from_hub` is True but `model_instance` is None, or if `from_hub` is False but `from_path` is None, or if
                        the loaded model is a state dict instead of a full model.
        """
        self.device = device

        if from_hub:
            if model_instance is not None:
                self.model = model_instance(pretrained=True).to(self.device)
            else:
                raise ValueError("You specified loading from a hub but did not provide a model instance.")

        else:
            if from_path:
                self.model = torch.load(from_path, map_location=self.device, weights_only=False)

                # Ensure that the loaded object is a full model, not a state dict
                if isinstance(self.model, dict):
                    raise ValueError("The loaded file appears to be a state dict. Please load a full model instead.")
            else:
                raise ValueError("You specified loading from a path but did not provide a path.")


    def get_model(self):
        """
        Get the model instance.

        Returns:
            torch.nn.Module: The model instance.
        """
        return self.model

class Trainer:
    def __init__(self, model, optimizer, criterion, train_loader, val_loader, test_loader, scheduler=None, device="cuda"):
        """
            Initialize the Trainer class with the provided model, optimizer, criterion, data loaders, and scheduler.

            Parameters:
                model (torch.nn.Module): The model to be trained and evaluated.
                optimizer (torch.optim.Optimizer): The optimizer to use for model training.
                criterion (callable): The loss function used during training and validation.
                train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.
                val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.
                test_loader (torch.utils.data.DataLoader): DataLoader for the testing dataset.
                scheduler (torch.optim.lr_scheduler._LRScheduler, optional): Learning rate scheduler for adjusting the learning rate during training.
                device (str, optional): Device to use for computations ('cuda' or 'cpu'). Default is 'cuda'.

            Attributes:
                model (torch.nn.Module): The model to be trained and evaluated.
                optimizer (torch.optim.Optimizer): The optimizer used for updating the model parameters.
                criterion (callable): The loss function used for computing loss.
                train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.
                val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.
                test_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.
                scheduler (torch.optim.lr_scheduler._LRScheduler, optional): Learning rate scheduler.
                device (str): The device used for computations ('cuda' or 'cpu').
                train_losses (list): List to store training losses for each epoch.
                val_losses (list): List to store validation losses for each epoch.
                train_accuracies (list): List to store training accuracies for each epoch.
                val_accuracies (list): List to store validation accuracies for each epoch.
                best_val_loss (float): Tracks the best validation loss achieved during training.
                best_model_path (str): Path to save the best model based on validation loss.
        """

        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.scheduler = scheduler
        self.device = device

        self.model.to(self.device)
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []

        self.best_val_loss = float('inf')
        self.best_model_path = 'best_model.pth'

    def train_epoch(self):
        """
            Train the model for one epoch on the training data.

            This method performs a single pass over the training dataset. It computes the loss, performs backpropagation, and updates the model's parameters. The method also tracks the running loss and accuracy for the entire training set.

            Returns:
                tuple: A tuple containing:
                    - epoch_loss (float): The average training loss over the epoch.
                    - epoch_acc (float): The accuracy of the model on the training dataset for the epoch.

            Notes:
                - The model is set to training mode using `self.model.train()`.
                - The optimizer gradients are zeroed before each backward pass using `self.optimizer.zero_grad()`.
                - The loss is backpropagated using `loss.backward()` and the optimizer step is called using `self.optimizer.step()`.
                - Loss and accuracy are accumulated for the entire dataset and returned as averages.
        """

        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in tqdm(self.train_loader, desc='Training', unit='batch'):
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            self.optimizer.zero_grad()

            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        epoch_loss = running_loss / len(self.train_loader.dataset)
        epoch_acc = correct / total

        return epoch_loss, epoch_acc

    def validate_epoch(self):
        """
            Validate the model for one epoch on the validation data.

            This method performs a single pass over the validation dataset. It computes the loss and accuracy without updating the model's parameters. The method tracks the running validation loss and accuracy over the entire validation set.

            Returns:
                tuple: A tuple containing:
                    - epoch_loss (float): The average validation loss over the epoch.
                    - epoch_acc (float): The accuracy of the model on the validation dataset for the epoch.

            Notes:
                - The model is set to evaluation mode using `self.model.eval()`.
                - The `torch.no_grad()` context is used to avoid calculating gradients during validation.
                - Loss and accuracy are accumulated for the entire validation dataset and returned as averages.
        """

        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in tqdm(self.val_loader, desc='Validating', unit='batch'):
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                running_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

        epoch_loss = running_loss / len(self.val_loader.dataset)
        epoch_acc = correct / total

        return epoch_loss, epoch_acc

    def train(self, num_epochs, use_early_stopping=False, early_stopping_patience=5):
        """
            Train the model for a specified number of epochs, with optional early stopping.

            This method trains the model for a given number of epochs, tracks the training and validation losses and accuracies, and applies a learning rate scheduler if provided. It saves the model with the best validation loss and optionally implements early stopping to prevent overfitting.

            Parameters:
                num_epochs (int): The number of epochs to train the model.
                use_early_stopping (bool, optional): If True, early stopping is enabled. Default is False.
                early_stopping_patience (int, optional): The number of epochs with no improvement in validation loss before early stopping is triggered. Default is 5.

            Notes:
                - The function alternates between training (`train_epoch`) and validating (`validate_epoch`) for each epoch.
                - The model with the lowest validation loss is saved to `self.best_model_path`.
                - If a scheduler is provided, it adjusts the learning rate. If the scheduler is `ReduceLROnPlateau`, it will step based on validation loss. Otherwise, it steps normally.
                - Training progress is printed after each epoch, including the current learning rate if a scheduler is used.
                - Early stopping terminates the training loop if validation loss doesn't improve for a specified number of consecutive epochs.

            Returns:
                None
        """

        early_stopping_counter = 0

        for epoch in range(num_epochs):
            print(f"Epoch {epoch+1}/{num_epochs}")

            train_loss, train_acc = self.train_epoch()
            val_loss, val_acc = self.validate_epoch()

            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)

            if val_loss < self.best_val_loss:
                print(f"Validation loss improved from {self.best_val_loss:.4f} to {val_loss:.4f}. Saving model...")
                self.best_val_loss = val_loss
                torch.save(self.model, self.best_model_path)
                early_stopping_counter = 0
            else:
                early_stopping_counter += 1

            if self.scheduler is not None:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()

                current_lr = scheduler.get_last_lr()[0]
                print(f"Current learning rate: {current_lr}")

            if use_early_stopping and early_stopping_counter >= early_stopping_patience:
                print(f"Early stopping triggered after {epoch+1} epochs.")
                break

            print(f"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}")
            print(f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}")

    def test(self, model_pth=None):
        """
            Evaluate the model on the test dataset and compute accuracy and F1 score.

            This method tests the model's performance on the provided test dataset. It calculates and prints the accuracy and F1 score of the model's predictions.

            Parameters:
                model_pth (str, optional): Path to the model file to load. If not provided, the current model is used.

            Returns:
                tuple: A tuple containing:
                    - test_accuracy (float): The accuracy of the model on the test dataset, expressed as a percentage.
                    - test_f1_score (float): The F1 score of the model on the test dataset, averaged over all classes.

            Notes:
                - If `model_pth` is provided, the model is loaded from this path; otherwise, the current model is evaluated.
                - The function sets the model to evaluation mode using `self.model.eval()`.
                - `torch.no_grad()` is used to prevent gradient calculations during testing.
                - The accuracy is computed as the percentage of correctly predicted samples.
                - The F1 score is calculated using the weighted average, considering the class imbalance.
        """

        if not model_pth:
            self.model = torch.load(model_pth)
        self.model.eval()
        correct = 0
        total = 0
        all_labels = []
        all_preds = []

        with torch.no_grad():
            for inputs, labels in tqdm(self.test_loader, desc='Testing', unit='batch'):
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                _, predicted = torch.max(outputs.data, 1)

                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(predicted.cpu().numpy())

        test_accuracy = 100 * correct / total
        test_f1_score = f1_score(all_labels, all_preds, average='weighted')

        print(f"Test Accuracy: {test_accuracy:.2f}%")
        print(f"Test F1 Score: {test_f1_score:.4f}")

        return test_accuracy, test_f1_score

    def plot_metrics(self):
        """
        Plot the training and validation losses and accuracies over epochs.
        """
        epochs = range(1, len(self.train_losses) + 1)

        # Plot Losses
        plt.figure(figsize=(14, 5))
        plt.subplot(1, 2, 1)
        plt.plot(epochs, self.train_losses, 'r', label='Training loss')
        plt.plot(epochs, self.val_losses, 'b', label='Validation loss')
        plt.title('Training and Validation Loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()

        # Plot Accuracies
        plt.subplot(1, 2, 2)
        plt.plot(epochs, self.train_accuracies, 'r', label='Training accuracy')
        plt.plot(epochs, self.val_accuracies, 'b', label='Validation accuracy')
        plt.title('Training and Validation Accuracy')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()

        plt.show()

#model_loader = ModelLoader(from_hub=True, model_instance=models.resnet34)
#model = model_loader.get_model()

#model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
#model.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)

#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#criterion = torch.nn.CrossEntropyLoss()
#train_loader, val_loader, test_loader = prep(dataset_name='mnist', domain='image', batch_size=128, device='cuda')

# Define a scheduler
#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)

# Initialize the trainer
#trainer = Trainer(
#    model=model,
#    optimizer=optimizer,
#    criterion=criterion,
#    train_loader=train_loader,
#    val_loader=val_loader,
#    test_loader=test_loader,
#    device='cuda',
#    scheduler=scheduler,
#)

# Train the model
#trainer.train(50, True, 5)

#trainer.plot_metrics()

#trainer.test("/kaggle/working/best_model.pth")
